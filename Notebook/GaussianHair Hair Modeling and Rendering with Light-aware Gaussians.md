# GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians

## 一、简介：

- 头发建模在过去二十年里借助神经建模和渲染的进步，从显示重建发展到隐式重建，但是构建这样的精细模型要不然需要建模工程师帮忙，要不然就得使用昂贵的相机设备搭配精心设计的可控环境场景，所有这些方法仅限于实验室场景。
- 隐式表达无法编辑照明条件，无法制作动态头发重建场景，与传统CG Pipeline不方便兼容。最新进展出现了混合表示方式，将外观特征转换为三维代理模型，如线和点[62,70]，或转换为更粗糙的几何形状[67]。（这里3DGS使用带球谐函数参数的可编辑高斯体具有优势）然后将这些代理模型渲染成二维，再通过神经渲染技术将其复杂地解码为头发外观，这里特别提到了神经链技术，即将头皮上编码为UV纹理的特征向量解码为与特定头皮位置相对应的精细发束，然后使用从合成数据集中学习的先验进行绞合级头发重建。最终外观渲染过程涉及大量神经渲染工作，（或许是计算开销太大、耗时太长、动态变换场景很费劲）限制了制作动态头发重建场景的可能性
- 3DGS使用带球谐函数参数的可编辑高斯体非常灵活，能实现仅使用手持设备进行视频输入就能达到生动的头发渲染（可动态实现）、编辑、重新照明、的效果，并且与传统的CG Pipeline无缝集成。
- 作者修改了初始化时3DGS的基本形状,将其修改为圆柱形3D高斯体。圆柱形高斯体具备三维高斯特征，其半径相对于其长度要小得多，方便将发束描绘成一系列链接的圆柱形三维高斯基本体。
- 作者先初始化了一组具有固定小半径的圆柱形3D高斯，从头部和头发网格上的点采样（包括建立RealHair dataset和LetsGo一样的招数，通讯作者有几个重复，可能出自同一课题组）通过可微分光栅化对其进行优化。再用Neural Haircut的方法从预训练的发束解码器生成粗发束，将圆柱形3D高斯分布分配给粗发束的两个相邻节点之间的每个部分，从而创建一个连接序列。之后再细化GaussianHair模型，（编码器-解码器）

## 二、相关工作

### 1.显式建模：

- 早期工作采用将头发描述成某种形式的参数，如2D参数曲面、细线和广义圆柱体、多分辨率圆柱体或头发网格。但这些方法想要创造出自然发型依旧很复杂，得靠建模师傅的帮助。
- 之后许多方法试图采用MVS等技术重建，有直接尝试和基于3D体积的解决方案，以启发式方式生成线的，3D volume-based solutions that generate strands in a heuristic manner.后续的方法侧重于使用定制的采集设备，然后对采集的结果进行三角剖分，使其存在各种类型的线（如带状、细束和线），为下一步拟合点云作指导。LPMVS引入了基于线的PatchMatch MVS算法，以更可靠地重建实际发束旁边的点云。然后将点云连接成发束，在这基础上还有人用per-pixel light code 对每一像素的光照进行编码捕捉反照率的颜色和反射率。
- 再后来的方法在合成头发数据集上预先学习头发形状，但这些数据驱动的方法在很大程度上依赖于训练数据的种类和发型质量，现在的公共发型数据集很少，风格相对简单，和现实相去甚远

### 2.隐式建模

- 使用MLP将空间位置或特征映射到某些属性如符号距离（指从一个空间点到物体表面的最短距离，点在物体外部为正，内部为负）占用场（二值函数，表示空间位置是否被物体占用），辐射场（一个隐式方程，用于描述一个场景中每个点的光照和颜色信息），可以处理极其复杂的几何形状和复杂的外观细节，不需要建模师傅帮助建模。
- NeRF的后续变体可以用来重新照明，但重新照明无法表达出镜面反射和通过单个纤维的散射这样的细节，后续也有关于动态场景实时渲染重建的工作，渲染质量会受到体素大小的限制，会受到影响，（而且NeRF毕竟是NeRF，里面的MLP训练所花的时间不会短）

### 3.3DGS

- 但是数万到数十万根头发直接用3DGS的高斯体去表示会很庞大密集，而且生成的表示不容易去编辑设置一些动画，也不容易重新照亮。后来人们用在初始化时用各种方法，比如使用平面，网格，或者长方体作为高斯体的原始体，作者在这用一系列连接的圆柱形3D高斯线。这种表示的一个主要优点是人们可以很容易地为发束设置动画，并且允许编辑这些几何体。



## 三、基于3DGS的头发表示方法



- 把高斯体初始化为圆柱形3D高斯体，把它连接起来表示发束，每个高斯体对应发束的一段，每个高斯体都能优化成不同的形状，可以用来表示发束复杂的几何结构

- 初始化时的缩放矩阵S是对角矩阵，对角元素[d,d,S]分别表示圆柱体的直径和长度，S在这设置为远大于d，直径d设置为2.0e-4

- 文中提到用粒子p<sub>i</sub>=u<sub>i</sub>+s<sub>i</sub>**d<sub>i</sub>**/2，这里的粒子是一个抽象的数学点表示。每根发丝的弯曲、倾斜可以通过这些粒子来表示，粒子可以在渲染过程中灵活地应用不同的光照、反射、散射等物理现象。，u<sub>i</sub>是高斯体中心点位置，s<sub>i</sub>是发束长度，**d<sub>i</sub>**是发束的方向向量，粒子位置相当于高斯体端点位置，然后颜色渲染和原版3DGS公式一样，但是对不透明度，球谐函数，还有旋转矩阵、缩放矩阵作了优化。

  ![image-20250318171715897](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250318171715897.png)

- T<sub>i</sub> 是透明度，c<sub>i</sub>是球谐函数，w<sub>i</sub>是之前的点作的颜色贡献（可理解为不透明度），也就是一个权重，即有多少的光在这个点留下了

## 四、基于图像的高斯头发建模

### 1.加快数据采集和处理

- 用作者的框架可以避免使用传统方法中的昂贵的相机阵列和其他复杂的设备。作者用iPhone 15 Pro Max 4k*60FPS去拍摄头发，拍摄分两轮，一轮离人物的头部1m，向下倾斜30°，聚焦头发的上部，另一轮直接水平放置。两轮朝同一个方向扫，全面覆盖人物的可见头发和上半身。选择采集的80到100帧视频，用现成SFM校准相机内外参，对每一帧用一组Gabor滤波器（用于提取图像中的局部频率信息，尤其是边缘和纹理信息）来计算2D方向图（用来记录像素点的方向，即该像素附近纹理或边缘的主方向。这种方向是通过分析像素邻域的灰度变化来确定的），用于帮助描述头发的纹理方向，特别是针对每根头发的朝向。
- 作者用alpha A，一种预训练的抠图模型，从图像中提取头发并去除背景，Hair Mask 用于判断像素是否被头发占据的二值掩码，然后FLAME（Face and Language Model）使用头部模型拟合面部和头部模型进行建模，再优化两层Instant-NSR模型，（一种生成式模型，用于高效地生成和优化3D头发模型。专注于对头发的建模和渲染）在这里，优化该模型可以帮助生成更逼真的头发效果。以上过程把头发网格和身体网格分开处理，然后后续重建阶段可以先把这个作为几何先验，即为对头发、身体初始形状的一个估计（指导粗发束的初始创建）后续再优化



### 2.定向三维场

从重建好的头发和身体网格中采样生成一组微小的3D高斯圆柱体分布，后续优化过程保持r不变，用原版的3DGS去优化其他参数

![image-20250318192434056](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250318192434056.png)

这是方向损失函数，P<sub>i</sub> 表示重建的这组高斯体方向集在图像序列i上像素j的投影，和O<sub>i</sub>实际像素i上的2D方向集作点积，如果二者方向一致的话，Loss为0。这里作者的Loss函数把原文的λ参数省了。

![image-20250318193107450](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250318193107450.png)

通过渲染FLAME头发网格得到头发纹理，然后UV映射得到二维映射特征图，特征图每个像素包含纹理、几何结构信息，这些会贴在3D高斯体上，得到带纹理信息的3D高斯体，然后通过解码器得到粗发束，粗发束再去优化，解码过程会从FLAME头部网格H上的头皮区域均匀采样根点，这是发束点（发束和头皮的交点），这里的初步渲染FLAME应该还是原版的3DGS。

![image-20250318194120315](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250318194120315.png)

粗发束优化第一步排除头部内的高斯体。

![image-20250318200016717](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250318200016717.png)

- 取头皮区域的点p，找离它最近的高斯体O，计算二者的几何上的距离，（1-dp*do)是衡量二者的方向，方向一致这项为0。

- 然后加入发型扩散先验损失，即把头发网格和身体网格分开处理作为后续重建阶段的几何先验的损失。优化了头皮和发束的几何纹理（这些都是发生在解码之前的），对其解码获得粗发束几何结构。
- 然后设置生成N根发束，每根发束L=100个节点，N个固定的发根位于头皮，N∗（L−1）个圆柱体3D高斯分布，每个都包含可训练的参数，包括旋转、长度、颜色和不透明度。
- 把原版3DGS中的Adam优化器改成了SGD优化器，专门设置了一个针对旋转和尺度的指数衰减学习率

![image-20250319162956101](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319162956101.png)

- 把原版光度损失作为纹理损失

![image-20250319164050875](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319164050875.png)

- 把Alpha A得到的Alpha Map里的除了Hair Mask，还有 alpha mask，数值用于判断头发区域透明程度，头发区域的透明度接近 1即为完全不透明。用这个损失计算不透明度一致性

![image-20250319164556498](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319164556498.png)

- 用于控制每条发束上每个高斯体的不透明度变换，防止其不透明度突变。图中式子表示相邻两个高斯体的不透明度变化量

![image-20250319164735024](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319164735024.png)

- 同理这个用于约束相邻两个高斯体直径和长度，防止突变

![image-20250319164833463](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319164833463.png)

- 总损失函数为上述四个损失函数相加，这些损失函数是为了之后的消融实验设计的
- 针对头发遮盖身体问题，先训练渲染头部、身体的模型，然后固定不动了之后再去训练头发，不需要用到HairMask
- 设计一种逐股自适应密度控制策略，控制高斯体分裂，减少了对最终结果影响最小的冗余圆柱高斯的数量，减小计算开销，在具有高梯度值的区域中（秃头或是头发稀疏），复制发束

## 五、高斯头发散射模型

### 5.1散射参数化

![image-20250319171845939](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319171845939.png)

- 光线经过发束有反射，透射+透射，透射+反射+透射，这是头发纤维散射示意图，这些结构在**Marschner**有详细描述，作者集成了Marschner的头发模型，将其描述为一个函数，这里为了加快渲染速度，用的是UE4里面的Marschner模型近似版本

![image-20250319172448262](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319172448262.png)

![image-20250319172831001](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319172831001.png)

- 引入散射函数S描述从w<sub>i</sub>方向入射，w<sub>o</sub> 出射的光的分布，然后E是从w<sub>o</sub>方向观看时，各个方向w<sub>i</sub>光出射度衰减的累计结果
- M是发束长度上的分布，N是宽度上的分布，作者按照图a建立了坐标系来量化

### 5.2多重散射近似

- 计算光线在头发内部的衰减，以确定每根头发接收到的光照强度，并且选择不透明度贡献值大于0.5的高斯体来计算，通过累积高斯体的不透明度，来近似模拟光在头发中的衰减过程。，这用于计算直接散射的强度

  主要散射成分是从点光源到达这个高斯体的光，但经过其他发束散射过来的光也很重要，这种也要考虑发束对光线的衰减作用。

  ![image-20250319195422665](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319195422665.png)![image-20250319195606929](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319195606929.png)![image-20250319195804457](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319195804457.png)

  n是垂直于d并在由d和ω<sub>0</sub>定义的同一平面内的伪法线方向,L(b) 是基础颜色的辐射亮度，S<sub>local</sub>是局部散射的强度，b 代表的是基础颜色的亮度

- S主散射光贡献，S<sub>local</sub>局部散射光贡献，τ<sub>k</sub>个光源穿过头发后的透射率,K是总的光源数量，k表示第k个光源

## 六、结论

### 6.1实验结论

- 用许多点光源搭建了一个光罩，然后按前面方法拍摄视频，采集了一个RealHair Dataset的数据集，数据集里什么人种什么颜色什么类型的头发都有，并且用GPT-4来标注图像里人物的性别、大致年龄、头发长度范围和颜色
- 和各种先进方法在几何结构衡量参数上对比（这个基于3DGS改的，加入了几个损失函数，不透明度累计中加入了散射模型，把原版3DGS中的Adam优化器改成了SGD优化器，专门设置了一个针对旋转和尺度的指数衰减学习率，在训练策略和渲染方式上都没改，抠图去背景后网格化后再采样，应该是3DGS和它最接近，但3DGS原版存在伪影问题的）

![image-20250319200705648](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319200705648.png)

- 去除设计的纹理和不透明度损失函数做消融实验，去除纹理损失函数有明显伪影，去除不透明度损失函数发束不连续

![image-20250319201412924](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20250319201412924.png)



### 6.2应用

- 可以通过改变基除颜色，参照公式(19) ,可以改变重建发型颜色，高斯体的高度和宽度可以改变，就可以调整头发的长度和粗细。
- 可以支持重新照明（在渲染图像时，改变光照条件，模拟不同的光源），就可以应用在CG领域，这种显示表示由于支持编辑头发属性，可以用在CG种实现动态的效果

### 6.3限制

- 集成的UE4的近似版本是一个简化过的多重散射模型，UE4里采样近似BSDF参数化
- 粗糙度和反射指数是手动调整的，原文没见这两具体指啥？

### 6.4结论

- RealHair Dataset数据集
- GaussianHair模型